# Model serving with Fastgen

## Usage

Start a Fastgen server with the following command:

```bash
torchrun --nproc-per-node 2 -m serve.fgserve config=serve/configs/cwm.yaml checkpoint_dir=</path/to/cwm/checkpoint>
```

You can set generation hyperparameters in the [config file](configs/cwm.yaml) or as arguments to individual requests. Note that if the tokenizer isn't specified as a command line argument or in the config file, it will be inferred from the checkpoint directory with the name "tokenizer.model" (the [download script](../README.md#model-download) will format it as such).

Once the server has started, you can send requests using the openai Python library.

```python
from openai import OpenAI
client = OpenAI(base_url=f"http://localhost:5678", api_key="foo")
system_prompt = "You are a helpful AI assistant. You always reason before responding, using the following format:\n\n<think>\nyour internal reasoning\n</think>\nyour external response"
completion = client.chat.completions.create(
    model="cwm",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "Write a haiku about recursion in programming."},
    ],
    max_tokens=4196, # Will override parameters from configs/cwm.yaml
    extra_body={"reasoning": {"enabled": True}},
)

print(completion.choices[0].message.content)
```

You can also use `curl` like so:

```bash
curl http://localhost:5678/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "cwm",
    "messages": [
      {"role": "system", "content": "You are a helpful AI assistant. You always reason before responding, using the following format:\n\n<think>\nyour internal reasoning\n</think>\nyour external response"},
      {"role": "user", "content": "Write a haiku about recursion in programming."}
    ],
    "reasoning": {"enabled": true}
  }' | jq -r '.choices[0].message |.reasoning_content, .content'
```

* If reasoning is enabled, we inject `<think>\n` into the beginning of the assistant's response. This `<think>\n` is not shown to the user. Further, if `</think>` is subsequently generated by the model, we extract and store all generated reasoning in `reasoning_content`.
* Reasoning can be disabled by setting `{"reasoning": {"enabled": false}}` (capital `False` for OpenAI API). If disabling reasoning is desired, make sure to also use an appropriate system prompt such as "You are a helpful AI assistant.".
* Tensor parallelism degree is decided by picking a number of GPUs greater
  than 1 in the `torchrun` command.
* For non-chat completions, we provide a `/completions` endpoint, expecting a `prompt` argument, see [./fgserve.py](fgserve.py).

## Fastgen library

Fastgen is an LLM inference library featuring state-of-the-art techniques: batched inference, cuda graphs, paged attention, chunked prefills, host-side kv-cache, tensor parallelism, cpu/gpu profiling.

You can see it as a mini (~3kloc) vLLM that is easily imported into larger systems such as RL loops or synthetic data generation pipelines.

A standalone open-source release of Fastgen is available at [facebookresearch/fastgen](https://github.com/facebookresearch/fastgen); eventually `cwm/fastgen` and `facebookresearch/fastgen` will be consolidated in a single source of truth.
